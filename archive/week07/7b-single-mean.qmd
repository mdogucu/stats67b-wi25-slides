---
title: "Inference for Single Mean"
author: "Dr. Mine Dogucu"
format: 
  revealjs:
    footer: "[stats4cs.com](https://stats4cs.com)"
    slide-number: true
    incremental: true
    theme: ["../templates/slide-style.scss"]
    logo: "https://www.stats4cs.com/img/logo.png"
    title-slide-attributes: 
      data-background-image: "https://www.stats4cs.com/img/logo.png"
      data-background-size: 5%
      data-background-position: 50% 85%
    include-after-body: "../templates/clean_title_page.html"
---

```{r}
#| echo: false
options(scipen=0)
library(tidyverse)
library(scales)
library(openintro)
set.seed(84735)

theme_set(theme_gray(base_size = 18))

plot_norm_x <- function(value, mean, sd, end = 3){
  data_norm <-  data.frame(x = c(mean-(end*sd), mean+(end*sd)))
  
  data_line <- data.frame(x = value, y = dnorm(value, mean, sd), 
                          label = paste("x = ", value))
  ggplot(data_norm, 
         aes(x)) +
    stat_function(fun = dnorm, n = 1001, args = list(mean = mean, sd = sd)) + 
    ylab("") +
    geom_segment(data = data_line, aes(x = value, y = 0, xend = x, yend = y), color = "#e56646") +
    geom_text(data = data_line, aes(x = x, y = 0, label = label), color = "#e56646") +
    labs(title =  bquote("N("*mu == .(mean) ~ "," ~ sigma == .(sd)*")"))
}



plot_norm_z <- function(value, mean, sd, end = 3){
  
  data_norm <-  data.frame(x = c(mean-(end*sd), mean+(end*sd)))
  data_z <- data.frame(x = c(-end,end))

  
  data_line_z <- data.frame(z = (value-mean)/sd, y = dnorm((value-mean)/sd), 
                            label = paste("z = ", (value-mean)/sd))
  
ggplot(data_z, 
               aes(x)) +
    stat_function(fun = dnorm, n = 1001, args = list(mean = 0, sd = 1)) + 
    xlab("z")+
    ylab("") +
    geom_segment(data = data_line_z, aes(x = z, y = 0, xend = z, yend = y), color = "#e56646") +
    geom_text(data = data_line_z, aes(x = z, y = 0, label = label), color = "#e56646") +
  labs(title =  bquote("N("*mu == 0 ~ "," ~ sigma == 1*")"))

  
}


plot_norm_x_prob <- function(value, mean, sd, end = 3, lower = TRUE){
  
  x <- seq(mean-(end*sd),mean+(end*sd), 0.01)
  
  y <- dnorm(x, mean = mean, sd = sd)
  
  data <- data.frame(x = x, y = y)
  
  data_line <- data.frame(x = value, y = dnorm(value, mean, sd), 
                          label = paste("x = ", value))
  
  if (lower == TRUE){
    p <- qplot(x, y, data = data, 
          geom = "line") +
      geom_ribbon(data = subset(data, x > (mean-(end*sd)) & x < value),
                  aes(ymax = y), 
                  ymin = 0,
                  fill = "#e56646", 
                  colour = NA, 
                  alpha = 0.5) +
      ylab("") 

  }
  
  if (lower == FALSE){
    p <- qplot(x, y, data = data, 
          geom = "line") +
      geom_ribbon(data = subset(data, x < (mean+(end*sd)) & x > value),
                  aes(ymax = y), 
                  ymin = 0,
                  fill = "#e56646", 
                  colour = NA, 
                  alpha = 0.5) +
      ylab("")
  }
  
p
}


```


## Uses of Normal Distribution

- SAT scores are normally distributed we were able to use normal distribution.

. . .

- Baby weights are normally distributed we were able to use normal distribution.

. . .

- According to Central Limit Theorem, sample proportions are approximately normally distributed (if certain conditions are met).

. . .

- According to Central Limit Theorem, difference of two proportions are approximately normally distributed (if certain conditions are met).



## What about Single Mean?


- According to Central Limit Theorem, sample means are approximately normally distributed (if certain conditions are met).

. . .


## Conditions

We can rely on Central Limit Theorem in order to make inference for a single mean if the following conditions have been met 

__Independence__ The sample data must be independent.

__Normality__ If the sample size is small, then the sample data must come from a population with a normal distribution. 


## Using CLT for Single Mean

If the conditions are met then

$\bar x \sim \text{approximately }N(mean = \mu, sd = \frac{\sigma}{\sqrt{n}})$

We have a problem! We do not know $\sigma$ so we may not calculate the standard error easily. 


## How did we handle this problem before?

When making inference for single proportion and difference of two proportions

$p \sim \text{approximately } N(mean = \pi, sd = \sqrt{\frac{\pi(1-\pi)}{n}})$

$(p_1 - \hat p_2) \sim \text{approximately } N((mean = \pi_1 - \pi_2), sd = \sqrt{\frac{\pi_1(1-\pi_1)}{n_1} + \frac{\pi_2(1-\pi_2)}{n_2}})$

we did not know the population proportions. How did we deal with this problem when calculating standard error?


## How did we handle this problem before?

We used the sample proportions. 


## How can we handle our current problem?

Since we do not know $\sigma$, the population standard deviation, we will instead use $s$, the sample standard deviation.

Using $s$ instead of $\sigma$ creates a problem in estimation.

Rather than using Normal distribution, we will now rely on $t$-distribution.


## t-distribution

```{r warning = FALSE}
ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dt, n = 101, args = list(df = 2)) + 
  scale_y_continuous(breaks = NULL) +
  ylab("f(x)") +
  theme(text = element_text(size = 20))

```

## t-distribution vs Normal Distribution

```{r warning = FALSE}

ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), aes(color = "z"))  +
  stat_function(fun = dt, n = 101, args = list(df = 1), aes(color = "t")) + 
  ylab("f(x)") +
  scale_y_continuous(breaks = NULL) +
  scale_colour_manual(name="Distribution",
  values=c(z="red", t="blue")) +
  theme(text = element_text(size = 20))


```

## t-distribution

In $t$-distribution, we will use one parameter only which is named degrees of freedom. 

$df = n - 1$


## t-distribution with different df

```{r warning = FALSE}

ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(fun = dt, n = 101, args = list(df = 1), aes(color = "df1")) + 
  stat_function(fun = dt, n = 101, args = list(df = 2), aes(color = "df2")) + 
  stat_function(fun = dt, n = 101, args = list(df = 4), aes(color = "df4")) + 
  stat_function(fun = dt, n = 101, args = list(df = 7), aes(color = "df7")) + 
  stat_function(fun = dt, n = 101, args = list(df = 10), aes(color = "df9")) + 
  ylab("f(x)") +
  scale_y_continuous(breaks = NULL) +
  scale_colour_manual(name="df",
  values=c(df1 = "red", df2="blue", df4 = "green", df7 = "black", df9 = "orange")) +
  theme(text = element_text(size = 20))


```

## t-distribution vs Normal Distribution

Note that as degrees of freedom increases t-distribution approaches a normal distribution. 


## t-distribution using R

```{r fig.height=3}
x <- seq(-4,4, 0.01)
y <- dt(x, df = 1)
data <- data.frame(x = x, y = y)
qplot(x, y, data = data, 
      geom = "line")+
  geom_ribbon(data = subset(data, x >- 4 & x < -2),
              aes(ymax = y), ymin = 0,
              fill = "#e56646", colour = NA, alpha = 0.5)+
  scale_y_continuous(limits = c(0, .4)) +
  ylab("f(x)") 

```


What proportion of values are below -2 in this distribution? 

$X \sim t_{df = 1} \text{ then }P(x \leq -2)$

```{r echo = TRUE}
pt(-2, df = 1) 
```


## t-distribution using R

```{r fig.height=3}
x <- seq(-4,4, 0.01)
y <- dt(x, df = 1)
data <- data.frame(x = x, y = y)
qplot(x, y, data = data, 
      geom = "line")+
  geom_ribbon(data = subset(data, x >- 4 & x < -2),
              aes(ymax = y), ymin = 0,
              fill = "#e56646", colour = NA, alpha = 0.5)+
  scale_y_continuous(limits = c(0, .4)) +
  ylab("f(x)") 

```


Below which value does the 14.75836% of the data lie?

```{r echo = TRUE}
qt(0.1475836, df = 1) #
```



## Where does the middle 95% of the data lie when $df = 14$? 

```{r fig.height=3}
x <- seq(-4,4, 0.01)
y <- dt(x, df = 14)
data <- data.frame(x = x, y = y)
qplot(x, y, data = data, 
      geom = "line")+
  geom_ribbon(data = subset(data, x >-2.144787 & x < 2.144787),
              aes(ymax = y), ymin = 0,
              fill = "#e56646", colour = NA, alpha = 0.5) +
  scale_y_continuous(limits = c(0, .4)) +
  ylab("f(x)") +
  theme(text = element_text(size = 20))

```

```{r echo = TRUE}
qt(0.025, df = 14)
```

95% of the data lies between -2.144787 and 2.144787



## Where does the middle 90% of the data lie when $df = 149$? 

```{r fig.height=3}
x <- seq(-4,4, 0.01)
y <- dt(x, df = 149)
data <- data.frame(x = x, y = y)
qplot(x, y, data = data, 
      geom = "line")+
  geom_ribbon(data = subset(data, x > -1.655145 & x < 1.655145),
              aes(ymax = y), ymin = 0,
              fill = "#e56646", colour = NA, alpha = 0.5) +
  scale_y_continuous(limits = c(0, .4)) +
  ylab("f(x)") +
  theme(text = element_text(size = 20))

```

```{r echo = TRUE}
qt(0.05, df = 149)
```

90% of the data lies between `r qt(0.05, df = 149)` and `r qt(0.05, df = 149)*-1`



# Confidence Interval

## Confidence Interval Review

Confidence Interval = point estimate $\pm$ critical value $\times$ standard error of the estimate

|                                           | point estimate | critical value | standard error of the estimate                    |
|-------------------------------------------|----------------|:--------------:|---------------------------------------------------|
| single proportion                         | $p$            | z*             | $\sqrt{\frac{p(1-p)}{n}}$                         |
| difference of two proportions             | $p_1-p_2$      | z*             | $\sqrt{\frac{p_1(1-p_1)}{n}+\frac{p_2(1-p_2)}{n}}$ |



## Confidence Interval for Single Mean

point estimate $\pm$ critical value $\times$ standard error


$\bar x$ $\pm$ critical value $\times$ standard error

. . .


$\bar x \pm t^*_{df}\times$ standard error

. . .

$\bar x \pm t^*_{df} \times \frac{s}{\sqrt n}$


## Exercise  


Prof. Garcia-Maldonado would like to know his average daily water intake. He records his water intake in his fitness app on his phone on 50 random days. His app shows that on average he drank 3.7 liters of water daily with standard deviation of 0.5 liters. Calculate a 95% confidence interval for average daily water intake of Prof. Garcia-Maldonado.


## Conditions

__Independence__ Despite the fact that Prof. Garcia-Maldonado has randomly selected the days he recorded data there may be possible violation of the independence condition. Consider that he selected 50 days in a two month frame. There will be many data points that are collected on consecutive days. If he has certain patterns of water drinking in a week then consecutive day data would be dependent and thus the condition of independence wuld be violated. However if he selected 50 days in a two year time frame it is possible that the data may meet the independence condition.


##

__Normality__ The sample size is greater than 30. Are there any outliers in the data?





## Known

$\bar x$ = 3.7

$s = 0.5$

$n = 50$







## Confidence Interval

$\bar x \pm t^*_{df} \times \frac{s}{\sqrt n}$




$3.7 \pm t^*_{df} \times \frac{0.5}{\sqrt{50}}$


## Finding critical value for 95% CI when $n = 50$

```{r fig.height=3}
x <- seq(-4,4, 0.01)
y <- dt(x, df = 49)
data <- data.frame(x = x, y = y)
qplot(x, y, data = data, 
      geom = "line")+
  geom_ribbon(data = subset(data, x > -2.009575 & x < 2.009575),
              aes(ymax = y), ymin = 0,
              fill = "red", colour = NA, alpha = 0.5) +
  scale_y_continuous(limits = c(0, .4)) +
  xlab("t") +
  ylab("f(t)") +
  theme(text = element_text(size = 20))

```
The critical value is:

```{r echo = TRUE}
qt(0.975, df = 49)
```


## Confidence Interval

$3.7 \pm 2.009575 \times \frac{0.5}{\sqrt{50}}$


```{r echo = TRUE}
se <- 0.5/sqrt(50)
critical_value <- qt(0.975, df = 49) 

3.7 - critical_value*se #lower bound
3.7 + critical_value*se #upper bound
```

##

We are 95% confident that the average daily water in take of Prof. Garcia- Maldonado is between 3.557902 and 3.842098 liters. 


# Hypothesis Testing

## Steps

1. Set hypotheses
2. Identify Sampling Distribution of $H_0$  
3. Calculate p-value  
4. Make a Decision and a Conclusion.


## Review

To calculate the p-value

1. We assume the null hypothesis is true.  
2. Based on the null hypothesis we plot/imagine the sampling distribution.  
3. We then examine where the sample statistic falls in this distribution.  
4. We then calculate the probability (p-value) of observing a sample statistic that is at least as extreme as the one that we have observed if the null hypothesis were true.

##

```{r echo = FALSE}
acs_employed <- acs12 %>% 
  filter(employment == "employed") 
acs_summary <- acs_employed %>% 
  summarize(mean_work = mean(hrs_work), 
            sd_work = sd(hrs_work))

```

The American Community Survey (ACS) is an ongoing survey by the U.S. Census Bureau that is conducted to gather information on people residing in the United States. We would like to know whether those who are employed work 40 hours a week on average. The ACS results in 2012 indicate that among the 843 people who were employed, the average work hour per week was `r acs_summary$mean_work` hours and standard deviation was `r acs_summary$sd_work` hours.


. . .

$\bar x = 38.9312$  
$s = 12.91513$  
$n = 843$  



## Conditions

**Independence** We are not given enough information about the sampling method used for this survey. Since the aim of the survey is to gather information from people in the US and given that the survey is conducted by US Census we would expect some form of randomized sampling. However, there are sampling strategies such as cluster sampling. In such cases, a family may be selected to be surveyed. In this case, the data would not meet the independence condition. We should check US Census website to gather further information on sampling strategies. For now, we will assume independence.

##

**Normality** Sample size > 30, outliers?



## Steps

1. Set hypotheses
2. Identify Sampling Distribution of $H_0$  
3. Calculate p-value  
4. Make a Decision and a Conclusion.



## Step 1 : Set hypotheses


$H_0: \mu = 40$  
$H_A: \mu \neq 40$



## Step 2: Sampling Distribution of $H_0$  

When certain __conditions__ are met then:


$\bar x \sim \text{approximately } N(mean = \mu, sd =\frac{\sigma}{\sqrt{n}})$

. . .

We will assume $H_0$ is true

$\bar x \sim \text{approximately } N( \text{mean} = 40,  \frac{12.91513}{\sqrt{843}})$

. . .

$\bar x \sim \text{approximately } N(40, \text{sd} = 0.4448207)$


## Step 3: Calculate p-value  

How many standard deviations (standard errors) is the point estimate away from the mean in the sampling distribution of the null hypothesis? 

. . .

$t = \frac{\bar x - \text{null}}{se}$


$t = \frac{38.9312-40}{\frac{12.91513}{\sqrt{843}}} = -2.402766$


## Step 3: Calculate p-value  

```{r, fig.height=4, fig.align='center'}

z <- seq(-3.5, 3.5, by = 0.0001)

t <- seq(-3.5, 3.5, by = 0.0001)

y <- dt(z, df = 842)

data <- tibble(t = t, y = y)

qplot(t, y, data = data, 
            geom = "line") +
        ylab("") +
        geom_ribbon(data = subset(data,  t < -2.402766),
                    aes(ymax = y), 
                    ymin = 0,
                    fill = "darkturquoise", 
                    colour = NA, 
                    alpha = 0.5) +
      
        geom_ribbon(data = subset(data,  t > 2.402766),
                    aes(ymax = y), 
                    ymin = 0,
                    fill = "darkturquoise", 
                    colour = NA, 
                    alpha = 0.5) +
        geom_vline(xintercept =-2.402766, color = "darkslateblue") +
        labs(title = "Null Distribution (t)") 
```

```{r echo = TRUE}
pt(-2.402766, df = 842) * 2
```


## Step 4: Make a Decision and a Conclusion.

If the null hypothesis were true ( $\mu = 40$ ) then probability of observing a sample statistic that is at least as extreme as the one that we have observed ( $\bar x = 38.9312$ ) would be `r pt(-2.402766, df = 842) * 2`. We consider this to be an evidence against the null since p-value (`r pt(-2.402766, df = 842) * 2`) is less than 0.05. In other words, if the null were true, it would be quite unlikely to observe a sample mean that is at least that extreme but since we have observed this sample, it is unlikely that the null is true so we reject the null and conclude that the average work time in the US in 2012 differs significantly from 40.
